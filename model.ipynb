{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de vidÃ©os: 346\n",
      "FINAL DATASET\n",
      "X: (119000, 8, 2)\n",
      "Y: (119000, 12, 2)\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "ANNOT_DIR = \"annotations\"\n",
    "\n",
    "OBS_LEN = 8\n",
    "PRED_LEN = 12\n",
    "\n",
    "all_X = []\n",
    "all_Y = []\n",
    "\n",
    "video_files = sorted([f for f in os.listdir(ANNOT_DIR) if f.endswith(\".xml\")])\n",
    "print(\"Nombre de vidÃ©os:\", len(video_files))\n",
    "\n",
    "for vid in video_files:\n",
    "\n",
    "    ann_path = os.path.join(ANNOT_DIR, vid)\n",
    "    tree = ET.parse(ann_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    ped_tracks = [t for t in root.findall(\"track\")\n",
    "                  if t.attrib.get(\"label\") == \"pedestrian\"]\n",
    "\n",
    "    for track in ped_tracks:\n",
    "\n",
    "        boxes = sorted(track.findall(\"box\"), key=lambda b: int(b.attrib[\"frame\"]))\n",
    "\n",
    "        traj = []\n",
    "        for b in boxes:\n",
    "            cx = (float(b.attrib[\"xtl\"]) + float(b.attrib[\"xbr\"])) / 2\n",
    "            cy = (float(b.attrib[\"ytl\"]) + float(b.attrib[\"ybr\"])) / 2\n",
    "            traj.append((cx, cy))\n",
    "\n",
    "        traj = np.array(traj)\n",
    "\n",
    "        for i in range(len(traj) - OBS_LEN - PRED_LEN):\n",
    "            X = traj[i : i + OBS_LEN]\n",
    "            Y = traj[i + OBS_LEN : i + OBS_LEN + PRED_LEN]\n",
    "\n",
    "            all_X.append(X)\n",
    "            all_Y.append(Y)\n",
    "\n",
    "all_X = np.array(all_X)\n",
    "all_Y = np.array(all_Y)\n",
    "\n",
    "print(\"FINAL DATASET\")\n",
    "print(\"X:\", all_X.shape)\n",
    "print(\"Y:\", all_Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\Mac Planet\\Documents\\computer vision\\model_Jaad\\env\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m X = torch.tensor(X, dtype=torch.float32)\n\u001b[32m      4\u001b[39m Y = torch.tensor(Y, dtype=torch.float32)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mac Planet\\Documents\\computer vision\\model_Jaad\\env\\Lib\\site-packages\\torch\\__init__.py:281\u001b[39m\n\u001b[32m    277\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    279\u001b[39m         kernel32.SetErrorMode(prev_error_mode)\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Libraries can either be in\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# path/nvidia/lib_folder/lib or\u001b[39;00m\n\u001b[32m    288\u001b[39m     \u001b[38;5;66;03m# path/nvidia/cuXX/lib (since CUDA 13.0) or\u001b[39;00m\n\u001b[32m    289\u001b[39m     \u001b[38;5;66;03m# path/lib_folder/lib\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mac Planet\\Documents\\computer vision\\model_Jaad\\env\\Lib\\site-packages\\torch\\__init__.py:264\u001b[39m, in \u001b[36m_load_dll_libraries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    260\u001b[39m     err = ctypes.WinError(last_error)\n\u001b[32m    261\u001b[39m     err.strerror += (\n\u001b[32m    262\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m Error loading \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m or one of its dependencies.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    263\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    266\u001b[39m     is_loaded = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\Mac Planet\\Documents\\computer vision\\model_Jaad\\env\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "print(X.shape, Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\Mac Planet\\Documents\\computer vision\\model_Jaad\\env\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(torch.__version__)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(torch.cuda.is_available())  \u001b[38;5;66;03m# DOIT Ãªtre False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mac Planet\\Documents\\computer vision\\model_Jaad\\env\\Lib\\site-packages\\torch\\__init__.py:281\u001b[39m\n\u001b[32m    277\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    279\u001b[39m         kernel32.SetErrorMode(prev_error_mode)\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Libraries can either be in\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# path/nvidia/lib_folder/lib or\u001b[39;00m\n\u001b[32m    288\u001b[39m     \u001b[38;5;66;03m# path/nvidia/cuXX/lib (since CUDA 13.0) or\u001b[39;00m\n\u001b[32m    289\u001b[39m     \u001b[38;5;66;03m# path/lib_folder/lib\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mac Planet\\Documents\\computer vision\\model_Jaad\\env\\Lib\\site-packages\\torch\\__init__.py:264\u001b[39m, in \u001b[36m_load_dll_libraries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    260\u001b[39m     err = ctypes.WinError(last_error)\n\u001b[32m    261\u001b[39m     err.strerror += (\n\u001b[32m    262\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m Error loading \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m or one of its dependencies.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    263\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    266\u001b[39m     is_loaded = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\Mac Planet\\Documents\\computer vision\\model_Jaad\\env\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class JAADDataset(Dataset):\n",
    "    def __init__(self, data_dict, seq_len=15, img_size=224):\n",
    "        \"\"\"\n",
    "        data_dict : JAAD split (train/val/test)\n",
    "        seq_len   : fixed sequence length for GRU\n",
    "        img_size  : cropped pedestrian image size\n",
    "        \"\"\"\n",
    "\n",
    "        self.images = data_dict['image']          # list of sequences of image paths\n",
    "        self.bboxes = data_dict['bbox']           # list of sequences of bbox coords\n",
    "        self.centers = data_dict['center']        # list of sequences of (x_center, y_center)\n",
    "        self.vehicle_act = data_dict['vehicle_act']  # vehicle state per frame\n",
    "        self.labels = [int(x[0][0]) for x in data_dict['intent']]  # sequence label (0/1)\n",
    "\n",
    "        # sequence length\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # image transformation\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((img_size, img_size)),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        # used to normalize bbox and center\n",
    "        self.img_w, self.img_h = data_dict['image_dimension']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def pad_sequence(self, *seq_lists):\n",
    "        \"\"\"\n",
    "        Pads or trims sequences so all have seq_len elements.\n",
    "        \"\"\"\n",
    "        padded = []\n",
    "        for seq in seq_lists:\n",
    "            s = list(seq)\n",
    "            while len(s) < self.seq_len:\n",
    "                s.append(s[-1])    # repeat last frame\n",
    "            padded.append(s[:self.seq_len])\n",
    "        return padded\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # original sequences\n",
    "        imgs = self.images[idx]\n",
    "        bboxes = self.bboxes[idx]\n",
    "        centers = self.centers[idx]\n",
    "        veh = self.vehicle_act[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # pad or cut sequences\n",
    "        imgs, bboxes, centers, veh = self.pad_sequence(\n",
    "            imgs, bboxes, centers, veh\n",
    "        )\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # 1) IMAGE SEQUENCE (cropped pedestrian images)\n",
    "        # ----------------------------------------------------\n",
    "        frame_tensors = []\n",
    "        for img_path, box in zip(imgs, bboxes):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img = img.crop((box[0], box[1], box[2], box[3]))\n",
    "            img = self.transform(img)\n",
    "            frame_tensors.append(img)\n",
    "\n",
    "        # shape = (T, 3, H, W)\n",
    "        frames = torch.stack(frame_tensors)\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # 2) EXTRA FEATURES PER FRAME\n",
    "        # ----------------------------------------------------\n",
    "        features = []\n",
    "        for c, b, v in zip(centers, bboxes, veh):\n",
    "\n",
    "            # center normalized\n",
    "            cx = c[0] / self.img_w\n",
    "            cy = c[1] / self.img_h\n",
    "\n",
    "            # bbox size normalized\n",
    "            bw = (b[2] - b[0]) / self.img_w\n",
    "            bh = (b[3] - b[1]) / self.img_h\n",
    "\n",
    "            # FIX â†’ vehicle_act is inside a list like [0]\n",
    "            veh_val = v[0] if isinstance(v, list) else v\n",
    "\n",
    "            features.append([cx, cy, bw, bh, float(veh_val)])\n",
    "\n",
    "        # shape = (T, 5)\n",
    "        extra = torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # return everything\n",
    "        # ----------------------------------------------------\n",
    "        return frames, extra, torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = JAADDataset(train, seq_len=15)\n",
    "val_dataset   = JAADDataset(val, seq_len=15)\n",
    "test_dataset  = JAADDataset(test, seq_len=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,   # put >0 only if NOT using Jupyter/Windows\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_sequence(dataset, index=0, num_frames=6):\n",
    "    frames, extra, label = dataset[index]\n",
    "\n",
    "    print(\"Label:\", int(label.item()))\n",
    "    print(\"Extra features (first frames):\")\n",
    "    print(extra[:num_frames])\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        img = frames[i].permute(1,2,0)  # Tensor CHW â†’ HWC\n",
    "\n",
    "        plt.figure(figsize=(3,3))\n",
    "        plt.imshow(img)\n",
    "        plt.title(\n",
    "            f\"Frame {i}\\n\"\n",
    "            f\"center={extra[i][:2].tolist()}\\n\"\n",
    "            f\"bbox size={extra[i][2:4].tolist()}\\n\"\n",
    "            f\"veh_act={extra[i][4].item()}\"\n",
    "        )\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "show_sequence(train_dataset, index=0, num_frames=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class PedestrianIntentModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cnn_backbone: str = \"resnet18\",\n",
    "        cnn_pretrained: bool = False,\n",
    "        extra_feat_dim: int = 5,   # [cx, cy, bw, bh, vehicle_act]\n",
    "        gru_hidden_dim: int = 128,\n",
    "        gru_num_layers: int = 1,\n",
    "        dropout: float = 0.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # -------------------------\n",
    "        # 1) CNN backbone\n",
    "        # -------------------------\n",
    "        if cnn_backbone == \"resnet18\":\n",
    "            self.cnn = models.resnet18(pretrained=cnn_pretrained)\n",
    "            cnn_feat_dim = self.cnn.fc.in_features\n",
    "            self.cnn.fc = nn.Identity()  # remove final classifier\n",
    "        elif cnn_backbone == \"alexnet\":\n",
    "            self.cnn = models.alexnet(pretrained=cnn_pretrained)\n",
    "            # flatten output of AlexNet classifier input\n",
    "            cnn_feat_dim = self.cnn.classifier[1].in_features\n",
    "            # keep only feature extractor part\n",
    "            self.cnn.classifier = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                # you could add your own linear layer here if you want\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {cnn_backbone}\")\n",
    "\n",
    "        self.cnn_feat_dim = cnn_feat_dim\n",
    "\n",
    "        # -------------------------\n",
    "        # 2) GRU over time\n",
    "        #    input = CNN features + extra features\n",
    "        # -------------------------\n",
    "        self.gru_input_dim = cnn_feat_dim + extra_feat_dim\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.gru_input_dim,\n",
    "            hidden_size=gru_hidden_dim,\n",
    "            num_layers=gru_num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        # -------------------------\n",
    "        # 3) Classification head\n",
    "        # -------------------------\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_dim, gru_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(gru_hidden_dim, 1)  # binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, frames, extra):\n",
    "        \"\"\"\n",
    "        frames: (B, T, 3, H, W)\n",
    "        extra:  (B, T, 5)\n",
    "        returns: logits of shape (B, 1)\n",
    "                 (use BCEWithLogitsLoss)\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = frames.shape\n",
    "\n",
    "        # -------------------------\n",
    "        # CNN: process each frame\n",
    "        # -------------------------\n",
    "        # (B, T, C, H, W) -> (B*T, C, H, W)\n",
    "        frames_flat = frames.view(B * T, C, H, W)\n",
    "\n",
    "        cnn_feats = self.cnn(frames_flat)              # (B*T, cnn_feat_dim)\n",
    "        cnn_feats = cnn_feats.view(B, T, self.cnn_feat_dim)  # (B, T, cnn_feat_dim)\n",
    "\n",
    "        # -------------------------\n",
    "        # Concatenate with extra features\n",
    "        # -------------------------\n",
    "        # extra already (B, T, extra_feat_dim)\n",
    "        gru_input = torch.cat([cnn_feats, extra], dim=-1)  # (B, T, cnn_feat_dim + extra_dim)\n",
    "\n",
    "        # -------------------------\n",
    "        # GRU\n",
    "        # -------------------------\n",
    "        gru_out, h_n = self.gru(gru_input)  # gru_out: (B, T, H), h_n: (num_layers, B, H)\n",
    "\n",
    "        # use last hidden state (of last layer)\n",
    "        last_hidden = h_n[-1]  # (B, H)\n",
    "\n",
    "        # -------------------------\n",
    "        # Classification head\n",
    "        # -------------------------\n",
    "        x = self.dropout(last_hidden)\n",
    "        logits = self.fc(x)  # (B, 1)\n",
    "\n",
    "        return logits  # apply sigmoid outside if you want probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4, device=\"cuda\"):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n===== Epoch {epoch+1}/{num_epochs} =====\")\n",
    "\n",
    "        # ------------------------------\n",
    "        # TRAINING\n",
    "        # ------------------------------\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "\n",
    "        for frames, extra, labels in pbar:\n",
    "            frames = frames.to(device)\n",
    "            extra = extra.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(frames, extra).squeeze(1)   # (B,)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * labels.size(0)\n",
    "\n",
    "            # compute accuracy\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "            train_correct += (preds == labels.long()).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        train_loss /= train_total\n",
    "        train_acc = train_correct / train_total\n",
    "\n",
    "        # ------------------------------\n",
    "        # VALIDATION\n",
    "        # ------------------------------\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for frames, extra, labels in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "                frames = frames.to(device)\n",
    "                extra = extra.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                logits = model(frames, extra).squeeze(1)\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                val_loss += loss.item() * labels.size(0)\n",
    "\n",
    "                preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "                val_correct += (preds == labels.long()).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "\n",
    "        # ------------------------------\n",
    "        # SAVE BEST MODEL\n",
    "        # ------------------------------\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_crossing_intent_model.pth\")\n",
    "            print(\"ðŸ”¥ Saved new best model!\")\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model = PedestrianIntentModel(\n",
    "    cnn_backbone=\"resnet18\",\n",
    "    cnn_pretrained=True\n",
    ")\n",
    "\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=3,\n",
    "    lr=1e-4,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test_model(model, test_loader, device=\"cuda\"):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for frames, extra, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            frames = frames.to(device)\n",
    "            extra = extra.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(frames, extra).squeeze(1)\n",
    "            probs = torch.sigmoid(logits)\n",
    "\n",
    "            preds = (probs >= 0.5).long()\n",
    "\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().long().tolist())\n",
    "\n",
    "    return all_labels, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def evaluate_predictions(labels, preds):\n",
    "    print(\"\\n=== Confusion Matrix ===\")\n",
    "    print(confusion_matrix(labels, preds))\n",
    "\n",
    "    print(\"\\n=== Classification Report ===\")\n",
    "    print(classification_report(labels, preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# load best model\n",
    "model = PedestrianIntentModel()\n",
    "model.load_state_dict(torch.load(\"best_crossing_intent_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# run test\n",
    "labels, preds = test_model(model, test_loader, device=device)\n",
    "\n",
    "# metrics\n",
    "evaluate_predictions(labels, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "labels = np.array([d[2] for d in train_dataset])  # dataset returns (frames, extra, label)\n",
    "count_0 = np.sum(labels == 0)\n",
    "count_1 = np.sum(labels == 1)\n",
    "print(count_0, count_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "# Extract labels from train_dataset\n",
    "labels = np.array([train_dataset[i][2].item() for i in range(len(train_dataset))])\n",
    "\n",
    "# Count samples\n",
    "class_count_0 = 32\n",
    "class_count_1 = 230\n",
    "\n",
    "# Compute weights\n",
    "weights = np.zeros_like(labels, dtype=float)\n",
    "weights[labels == 0] = 1.0 / class_count_0\n",
    "weights[labels == 1] = 1.0 / class_count_1\n",
    "\n",
    "# Build sampler\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=weights,\n",
    "    num_samples=len(labels),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# Build train loader with sampler\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    sampler=sampler,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Balanced sampler created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model = PedestrianIntentModel(\n",
    "    cnn_backbone=\"resnet18\",\n",
    "    cnn_pretrained=True\n",
    ")\n",
    "\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=3,\n",
    "    lr=1e-4,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# load best model\n",
    "model = PedestrianIntentModel()\n",
    "model.load_state_dict(torch.load(\"best_crossing_intent_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# run test\n",
    "labels, preds = test_model(model, test_loader, device=device)\n",
    "\n",
    "# metrics\n",
    "evaluate_predictions(labels, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "train = jaad.generate_data_trajectory_sequence(\n",
    "    'train',\n",
    "    seq_type='intention',\n",
    "    sample_type='all',\n",
    "    fstride=5\n",
    "\n",
    ")\n",
    "\n",
    "val = jaad.generate_data_trajectory_sequence(\n",
    "    'val',\n",
    "    seq_type='intention',\n",
    "    sample_type='all',\n",
    "    fstride=5\n",
    "\n",
    ")\n",
    "\n",
    "test = jaad.generate_data_trajectory_sequence(\n",
    "    'test',\n",
    "    seq_type='intention',\n",
    "    sample_type='all',\n",
    "    fstride=5\n",
    ")\n",
    "\n",
    "\n",
    "print(len(train['image']))\n",
    "print(train['image'][0])\n",
    "print(train['intent'][0])\n",
    "print(train['bbox'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "print(train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class JAADDataset(Dataset):\n",
    "    def __init__(self, data_dict, seq_len=15, img_size=224):\n",
    "        \"\"\"\n",
    "        For seq_type='intention'\n",
    "        JAAD gives: image, pid, bbox, center, occlusion, intent\n",
    "        \"\"\"\n",
    "\n",
    "        self.images  = data_dict[\"image\"]\n",
    "        self.bboxes  = data_dict[\"bbox\"]\n",
    "        self.centers = data_dict[\"center\"]\n",
    "        self.labels  = [int(x[0][0]) for x in data_dict[\"intent\"]]\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((img_size, img_size)),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        # WARNING: image_dimension does NOT exist in intention mode\n",
    "        # So we normalize using actual frame size from bbox\n",
    "        # (w = x2 - x1, h = y2 - y1)\n",
    "        # No global width/height available.\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def pad(self, seq):\n",
    "        \"\"\"Pad sequence to fixed seq_len.\"\"\"\n",
    "        seq = list(seq)\n",
    "        if len(seq) == 0:\n",
    "            return [seq[0]] * self.seq_len\n",
    "        while len(seq) < self.seq_len:\n",
    "            seq.append(seq[-1])\n",
    "        return seq[:self.seq_len]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imgs    = self.pad(self.images[idx])\n",
    "        bboxes  = self.pad(self.bboxes[idx])\n",
    "        centers = self.pad(self.centers[idx])\n",
    "        label   = self.labels[idx]\n",
    "\n",
    "        frame_tensors = []\n",
    "        extra_feats   = []\n",
    "\n",
    "        for img_path, box, center in zip(imgs, bboxes, centers):\n",
    "\n",
    "            # --------------------------\n",
    "            # 1) Load and crop image\n",
    "            # --------------------------\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            x1, y1, x2, y2 = box\n",
    "            img = img.crop((x1, y1, x2, y2))\n",
    "            img = self.transform(img)\n",
    "            frame_tensors.append(img)\n",
    "\n",
    "            # --------------------------\n",
    "            # 2) Frame-specific features\n",
    "            # --------------------------\n",
    "            cx, cy = center\n",
    "\n",
    "            # Normalize using bbox size (local normalization)\n",
    "            bw = (x2 - x1)\n",
    "            bh = (y2 - y1)\n",
    "            cx_rel = (cx - x1) / bw if bw > 0 else 0\n",
    "            cy_rel = (cy - y1) / bh if bh > 0 else 0\n",
    "            bw_rel = bw / 1920       # assume HD width\n",
    "            bh_rel = bh / 1080       # assume HD height\n",
    "\n",
    "            extra_feats.append([cx_rel, cy_rel, bw_rel, bh_rel])\n",
    "\n",
    "        frames = torch.stack(frame_tensors)                  # (T, 3, H, W)\n",
    "        extras = torch.tensor(extra_feats, dtype=torch.float32)  # (T, 4)\n",
    "\n",
    "        return frames, extras, torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = JAADDataset(train, seq_len=50)\n",
    "val_dataset   = JAADDataset(val, seq_len=50)\n",
    "test_dataset  = JAADDataset(test, seq_len=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "# Extract labels\n",
    "train_labels = np.array([train_dataset[i][2].item() for i in range(len(train_dataset))])\n",
    "\n",
    "count_0 = np.sum(train_labels == 0)\n",
    "count_1 = np.sum(train_labels == 1)\n",
    "print(count_0, count_1)\n",
    "\n",
    "# Inverse frequency = larger weight for rare class (0)\n",
    "weight_0 = 1.0 / count_0\n",
    "weight_1 = 1.0 / count_1\n",
    "\n",
    "sample_weights = np.array([weight_0 if l == 0 else weight_1 for l in train_labels])\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    sampler=sampler,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# val/test normal loaders\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "pos_weight = torch.tensor(count_1 / count_0)   # increases penalty for class 0 errors\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    \n",
    "    # class imbalance fixed here\n",
    "    train_labels = np.array([train_dataset[i][2].item() for i in range(len(train_dataset))])\n",
    "    count_0 = np.sum(train_labels == 0)\n",
    "    count_1 = np.sum(train_labels == 1)\n",
    "    pos_weight = torch.tensor(count_1 / count_0).to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n===== Epoch {epoch+1}/{num_epochs} =====\")\n",
    "\n",
    "        # ---------------------\n",
    "        # TRAIN MODE\n",
    "        # ---------------------\n",
    "        model.train()\n",
    "        total_loss, total_correct, total_samples = 0, 0, 0\n",
    "\n",
    "        for frames, extra, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            frames, extra, labels = frames.to(device), extra.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(frames, extra).squeeze(1)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "            total_correct += (preds == labels.long()).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "        train_loss = total_loss / total_samples\n",
    "        train_acc = total_correct / total_samples\n",
    "\n",
    "        # ---------------------\n",
    "        # VALIDATION MODE\n",
    "        # ---------------------\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_samples = 0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for frames, extra, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "                frames, extra, labels = frames.to(device), extra.to(device), labels.to(device)\n",
    "\n",
    "                logits = model(frames, extra).squeeze(1)\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                val_loss += loss.item() * labels.size(0)\n",
    "                preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "                val_correct += (preds == labels.long()).sum().item()\n",
    "                val_samples += labels.size(0)\n",
    "\n",
    "        val_loss /= val_samples\n",
    "        val_acc = val_correct / val_samples\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "\n",
    "        # SAVE BEST MODEL\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_crossing_intent_model.pth\")\n",
    "            print(\"ðŸ”¥ Saved new BEST model!\")\n",
    "\n",
    "    print(\"\\nTraining finished!\")\n",
    "    print(\"Best Validation Accuracy:\", best_val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model = PedestrianIntentModel(\n",
    "    cnn_backbone=\"resnet18\",\n",
    "    cnn_pretrained=True,\n",
    "    extra_feat_dim=4\n",
    ")\n",
    "\n",
    "train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=3,\n",
    "    lr=1e-4,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# load best model\n",
    "model = PedestrianIntentModel()\n",
    "model.load_state_dict(torch.load(\"best_crossing_intent_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# run test\n",
    "labels, preds = test_model(model, test_loader, device=device)\n",
    "\n",
    "# metrics\n",
    "evaluate_predictions(labels, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "model = PedestrianIntentModel(\n",
    "    cnn_backbone=\"resnet18\",   # or \"alexnet\"\n",
    "    cnn_pretrained=False       # True if you want pretrained weights\n",
    ")\n",
    "\n",
    "batch_frames, batch_extra, batch_labels = next(iter(train_loader))\n",
    "# batch_frames: (B, T, 3, 224, 224)\n",
    "# batch_extra:  (B, T, 5)\n",
    "# batch_labels: (B,)\n",
    "\n",
    "logits = model(batch_frames, batch_extra)  # (B, 1)\n",
    "\n",
    "# For training:\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "loss = criterion(logits.squeeze(1), batch_labels.float())\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
